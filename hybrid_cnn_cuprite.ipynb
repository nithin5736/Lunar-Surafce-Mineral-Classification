{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import scipy.io as sio\n",
    "import math\n",
    "from tensorflow.python.framework import ops\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_dataset(cuprite_dataset, cnv_dataset, target_labels):\n",
    "    num_spectral_bands = cuprite_dataset.shape[1]\n",
    "    cnv_dataset = cnv_dataset.reshape((cnv_dataset.shape[0], num_spectral_bands, 7, 7))\n",
    "    train_cuprite, test_cuprite, train_cnv, test_cnv, train_labels, test_labels = train_test_split(\n",
    "        cuprite_dataset, cnv_dataset, target_labels, test_size=0.2, random_state=42\n",
    "    )\n",
    "    train_cnv = train_cnv.reshape((train_cnv.shape[0], -1))\n",
    "    test_cnv = test_cnv.reshape((test_cnv.shape[0], -1))\n",
    "    return train_cuprite, test_cuprite, train_cnv, test_cnv, train_labels, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(input_data):\n",
    "    max_value = np.amax(input_data)\n",
    "    min_value = np.amin(input_data)\n",
    "    shifted_data = input_data + abs(min_value)\n",
    "    normalized_data = shifted_data / (abs(min_value) + abs(max_value))\n",
    "    normalized_data = normalized_data + 1e-6\n",
    "    return normalized_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mask(idx, l):\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_placeholders(n_x, n_x1, n_y):\n",
    "    training_bool = tf.compat.v1.placeholder_with_default(True, shape=())\n",
    "    inp_x = tf.compat.v1.placeholder(tf.float32, [None, n_x], name=\"inp_x\")\n",
    "    inp_x1 = tf.compat.v1.placeholder(tf.float32, [None, n_x1], name=\"inp_x1\")\n",
    "    inp_y = tf.compat.v1.placeholder(tf.float32, [None, n_y], name=\"inp_y\")\n",
    "    lap_train = tf.compat.v1.placeholder(tf.float32, [None, None], name=\"lap_train\")\n",
    "    return inp_x, inp_x1, inp_y, lap_train, training_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjacency_matrix(all_x, sigma_spatial=1.0, sigma_spectral=1.0):\n",
    "    n = all_x.shape[0]\n",
    "    spatial_distances = np.zeros((n, n))\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i != j:\n",
    "                spatial_distances[i, j] = np.linalg.norm(all_x[i, :2] - all_x[j, :2])\n",
    "    spectral_distances = rbf_kernel(all_x, gamma=1.0 / (2.0 * sigma_spectral**2))\n",
    "    A = np.exp(-((spatial_distances / sigma_spatial)**2 + spectral_distances) / 2.0)\n",
    "    return csr_matrix(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomize_data(input1, input2, labels, matrix, random_seed):\n",
    "    total_samples = input1.shape[0]\n",
    "    np.random.seed(random_seed)\n",
    "    randomized_indices = list(np.random.permutation(total_samples))\n",
    "    randomized_input1 = input1[randomized_indices, :]\n",
    "    randomized_input2 = input2[randomized_indices, :]\n",
    "    randomized_labels = labels[randomized_indices, :].reshape((total_samples, labels.shape[1]))\n",
    "    randomized_matrix1 = matrix[randomized_indices, :].reshape((matrix.shape[0], matrix.shape[1]), order=\"F\")\n",
    "    randomized_matrix = randomized_matrix1[:, randomized_indices].reshape((matrix.shape[0], matrix.shape[1]), order=\"F\")\n",
    "    return randomized_input1, randomized_input2, randomized_labels, randomized_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mini_batches(randomized_input1, randomized_input2, randomized_labels, randomized_matrix, mini_batch_size):\n",
    "    total_samples = randomized_input1.shape[0]\n",
    "    mini_batches = []\n",
    "    num_of_batches = math.floor(total_samples / mini_batch_size)\n",
    "    for i in range(0, num_of_batches):\n",
    "        start_index = i * mini_batch_size\n",
    "        end_index = start_index + mini_batch_size\n",
    "        mini_batch = (randomized_input1[start_index:end_index, :], randomized_input2[start_index:end_index, :],\n",
    "                      randomized_labels[start_index:end_index, :], randomized_matrix[start_index:end_index, start_index:end_index])\n",
    "        mini_batches.append(mini_batch)\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_min_btchs_gcn1(input1, input2, labels, matrix, mini_batch_size, random_seed):\n",
    "    randomized_input1, randomized_input2, randomized_labels, randomized_matrix = randomize_data(input1, input2, labels, matrix, random_seed)\n",
    "    mini_batches = create_mini_batches(randomized_input1, randomized_input2, randomized_labels, randomized_matrix, mini_batch_size)\n",
    "    mini_batch = (input1, input2, labels, matrix)\n",
    "    mini_batches.append(mini_batch)\n",
    "    return mini_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gcn_layer(inp_x, adj_matrix, weights):\n",
    "    x_mid = tf.matmul(inp_x, weights)\n",
    "    x_out = tf.matmul(adj_matrix, x_mid)\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_params():\n",
    "    tf.compat.v1.set_random_seed(1)\n",
    "    initializer = tf.compat.v1.keras.initializers.VarianceScaling(scale=1.0, mode=\"fan_avg\", distribution=\"uniform\", seed=1)\n",
    "    zeros_init = tf.compat.v1.zeros_initializer()\n",
    "\n",
    "    params = {\n",
    "        \"weight1_x\": tf.compat.v1.get_variable(\"weight1_x\", [224, 128], initializer=initializer),\n",
    "        \"bias1_x\": tf.compat.v1.get_variable(\"bias1_x\", [128], initializer=zeros_init),\n",
    "        \"jweight1_x\": tf.compat.v1.get_variable(\"jweight1_x\", [128 + 128, 128], initializer=initializer),\n",
    "        \"jbias1_x\": tf.compat.v1.get_variable(\"jbias1_x\", [128], initializer=zeros_init),\n",
    "        \"jweight2_x\": tf.compat.v1.get_variable(\"jweight2_x\", [128, 12], initializer=initializer),\n",
    "        \"jbias2_x\": tf.compat.v1.get_variable(\"jbias2_x\", [12], initializer=zeros_init),\n",
    "        \"cnv_weight1_x\": tf.compat.v1.get_variable(\"cnv_weight1_x\", [3, 3, 224, 32], initializer=initializer),\n",
    "        \"cnv_bias1_x\": tf.compat.v1.get_variable(\"cnv_bias1_x\", [32], initializer=zeros_init),\n",
    "        \"cnv_weight2_x\": tf.compat.v1.get_variable(\"cnv_weight2_x\", [3, 3, 32, 64], initializer=initializer),\n",
    "        \"cnv_bias2_x\": tf.compat.v1.get_variable(\"cnv_bias2_x\", [64], initializer=zeros_init),\n",
    "        \"cnv_weight3_x\": tf.compat.v1.get_variable(\"cnv_weight3_x\", [1, 1, 64, 128], initializer=initializer),\n",
    "        \"cnv_bias3_x\": tf.compat.v1.get_variable(\"cnv_bias3_x\", [128], initializer=zeros_init)\n",
    "    }\n",
    "\n",
    "    return params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_batch_normalization(input, is_training, momentums):\n",
    "    return tf.compat.v1.layers.batch_normalization(input, momentum=momentums, training=is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_convolution(input, filters, bias, strides=[1, 1, 1, 1], padding='SAME'):\n",
    "    return tf.nn.conv2d(input, filters=filters, strides=strides, padding=padding) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_max_pooling(input, pool_size, strides, padding='SAME'):\n",
    "    return tf.compat.v1.layers.max_pooling2d(input, pool_size, strides, padding=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_relu(input):\n",
    "    return tf.nn.relu(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gcn(input, laplacian, weight, bias):\n",
    "    return gcn_layer(input, laplacian, weight) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_matmul(input, weight, bias):\n",
    "    return tf.matmul(input, weight) + bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mynetwork(x_input, x1_input, params, laplacian, is_training, momentums=0.9):\n",
    "    x1_reshaped = tf.reshape(x1_input, [-1, 7, 7, 224], name=\"x1\")\n",
    "\n",
    "    with tf.compat.v1.name_scope(\"first_layer_x\"):\n",
    "        batch_normalization_xz1 = apply_batch_normalization(x_input, is_training, momentums)\n",
    "        gcn_xz1 = apply_gcn(batch_normalization_xz1, laplacian, params['weight1_x'], params['bias1_x'])\n",
    "        batch_normalization_xz1 = apply_batch_normalization(gcn_xz1, is_training, momentums)\n",
    "        x_a1 = apply_relu(batch_normalization_xz1)\n",
    "        cnv_xz1 = apply_convolution(x1_reshaped, params['cnv_weight1_x'], params['cnv_bias1_x'])\n",
    "        batch_normalization_cnv_xz1 = apply_batch_normalization(cnv_xz1, is_training, momentums)\n",
    "        pooling_cnv_xz1 = apply_max_pooling(batch_normalization_cnv_xz1, 2, 2)\n",
    "        cnv_xa1 = apply_relu(pooling_cnv_xz1)\n",
    "\n",
    "    with tf.compat.v1.name_scope(\"second_layer_x\"):\n",
    "        cnv_xz2 = apply_convolution(cnv_xa1, params['cnv_weight2_x'], params['cnv_bias2_x'])\n",
    "        batch_normalization_cnv_xz2 = apply_batch_normalization(cnv_xz2, is_training, momentums)\n",
    "        pooling_cnv_xz2 = apply_max_pooling(batch_normalization_cnv_xz2, 2, 2)\n",
    "        x_conv_a2 = apply_relu(pooling_cnv_xz2)\n",
    "\n",
    "    with tf.compat.v1.name_scope(\"third_layer_x\"):\n",
    "        cnv_xz3 = apply_convolution(x_conv_a2, params['cnv_weight3_x'], params['cnv_bias3_x'])\n",
    "        batch_normalization_cnv_xz3 = apply_batch_normalization(cnv_xz3, is_training, momentums)\n",
    "        pooling_cnv_xz3 = apply_max_pooling(batch_normalization_cnv_xz3, 2, 2)\n",
    "        cnv_xa3 = apply_relu(pooling_cnv_xz3)\n",
    "        cnv_xa3_shape = cnv_xa3.get_shape().as_list()\n",
    "        cnv_xz3_2d = tf.reshape(cnv_xa3, [-1, cnv_xa3_shape[1] * cnv_xa3_shape[2] * cnv_xa3_shape[3]])\n",
    "        jt_encd_layer = tf.concat([x_a1, cnv_xz3_2d], 1)\n",
    "\n",
    "    with tf.compat.v1.name_scope(\"joint_layer_1\"):\n",
    "        xzj1 = apply_matmul(jt_encd_layer, params['jweight1_x'], params['jbias1_x'])\n",
    "        xzj1_bn = apply_batch_normalization(xzj1, is_training, momentums)\n",
    "        xaj1 = apply_relu(xzj1_bn)\n",
    "\n",
    "    with tf.compat.v1.name_scope(\"x_layer_4\"):\n",
    "        x_zj2 = apply_matmul(xaj1, params['jweight2_x'], params['jbias2_x'])\n",
    "\n",
    "    l2_loss = (\n",
    "        tf.nn.l2_loss(params['weight1_x']) + tf.nn.l2_loss(params['jweight1_x']) +\n",
    "        tf.nn.l2_loss(params['jweight2_x']) + tf.nn.l2_loss(params['cnv_weight1_x']) +\n",
    "        tf.nn.l2_loss(params['cnv_weight2_x']) + tf.nn.l2_loss(params['cnv_weight3_x'])\n",
    "    )\n",
    "\n",
    "    return x_zj2, l2_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(predicted_output, actual_output, l2_loss_value, regularization):\n",
    "    actual_output = tf.squeeze(actual_output, name='actual_output')\n",
    "    with tf.compat.v1.name_scope(\"cost_calculation\"):\n",
    "        cost_value = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=predicted_output, labels=tf.stop_gradient(actual_output))) + regularization * l2_loss_value\n",
    "    return cost_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_update_operations():\n",
    "    with tf.compat.v1.name_scope(\"optimization_process\"):\n",
    "        update_operations = tf.compat.v1.get_collection(tf.compat.v1.GraphKeys.UPDATE_OPS)\n",
    "    return update_operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(cost_value, learning_rate_value, global_step_value, update_operations):\n",
    "    with tf.control_dependencies(update_operations):\n",
    "        optimizer_process = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate_value).minimize(cost_value, global_step=global_step_value)\n",
    "        optimizer_process = tf.group([optimizer_process, update_operations])\n",
    "    return optimizer_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_network(predicted_output, actual_output, l2_loss_value, regularization, learning_rate_value, global_step_value):\n",
    "    cost_value = calculate_cost(predicted_output, actual_output, l2_loss_value, regularization)\n",
    "    update_operations = get_update_operations()\n",
    "    optimizer_process = optimize(cost_value, learning_rate_value, global_step_value, update_operations)\n",
    "    return cost_value, optimizer_process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(loss_values, loss_values_dev, acc_values, acc_values_dev):\n",
    "    plt.plot(np.squeeze(loss_values))\n",
    "    plt.plot(np.squeeze(loss_values_dev))\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(np.squeeze(acc_values))\n",
    "    plt.plot(np.squeeze(acc_values_dev))\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(features, features1, labels):\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    tf.compat.v1.set_random_seed(1)\n",
    "    placeholder_input, placeholder_input1, placeholder_output, laplace_train, training_flag = create_placeholders(features, features1, labels)\n",
    "    params = initialize_params()\n",
    "    with tf.compat.v1.name_scope(\"network\"):\n",
    "        output_input, l2_loss_val = mynetwork(placeholder_input, placeholder_input1, params, laplace_train, training_flag)\n",
    "    return placeholder_input, placeholder_input1, placeholder_output, laplace_train, training_flag, output_input, l2_loss_val, params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_optimizer(output_input, placeholder_output, l2_loss_val, reg_beta, alpha_start, samples, batch_size):\n",
    "    step = tf.Variable(0, trainable=False)\n",
    "    learning_rate = tf.compat.v1.train.exponential_decay(\n",
    "        alpha_start, step, 50 * samples / batch_size, 0.5, staircase=True)\n",
    "    with tf.compat.v1.name_scope(\"optimization\"):\n",
    "        loss_val, optimizer_val = optimize_network(output_input, placeholder_output, l2_loss_val, reg_beta, learning_rate, step)\n",
    "    return loss_val, optimizer_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_metrics(output_input, placeholder_output):\n",
    "    with tf.compat.v1.name_scope(\"metrics\"):\n",
    "        joint_layer_transpose = tf.transpose(output_input)\n",
    "        output_transpose = tf.transpose(placeholder_output)\n",
    "        correct_pred = tf.equal(tf.argmax(joint_layer_transpose), tf.argmax(output_transpose))\n",
    "        accuracy_val = tf.reduce_mean(tf.cast(correct_pred, \"float\"))\n",
    "    return accuracy_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(sess, optimizer_val, loss_val, accuracy_val, minibatches, placeholder_input, placeholder_input1, placeholder_output, laplace_train, training_flag):\n",
    "    epoch_loss, epoch_accuracy = 0., 0.\n",
    "    num_minibatches = len(minibatches)\n",
    "    for minibatch in minibatches:\n",
    "        (batch_input, batch_input1, batch_output, batch_laplace) = minibatch\n",
    "        _, minibatch_loss, minibatch_accuracy = sess.run(\n",
    "            [optimizer_val, loss_val, accuracy_val],\n",
    "            feed_dict={placeholder_input: batch_input, placeholder_input1: batch_input1, placeholder_output: batch_output, laplace_train: batch_laplace, training_flag: True}\n",
    "        )\n",
    "        epoch_loss += minibatch_loss / num_minibatches\n",
    "        epoch_accuracy += minibatch_accuracy / num_minibatches\n",
    "    return epoch_loss, epoch_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(train_input, test_input, train_input1, test_input1, train_output, test_output, L_train_input, L_test_input, alpha_start=0.001,\n",
    "                reg_beta=0.001, total_epochs=200, batch_size=32, print_loss=True):\n",
    "    random_seed = 1\n",
    "    samples, features = train_input.shape\n",
    "    samples, labels = train_output.shape\n",
    "    samples, features1 = train_input1.shape\n",
    "    loss_values, loss_values_dev, train_accuracy, validation_accuracy = [], [], [], []\n",
    "\n",
    "    placeholder_input, placeholder_input1, placeholder_output, laplace_train, training_flag, output_input, l2_loss_val, params = create_model(features, features1, labels)\n",
    "    loss_val, optimizer_val = create_optimizer(output_input, placeholder_output, l2_loss_val, reg_beta, alpha_start, samples, batch_size)\n",
    "    accuracy_val = create_metrics(output_input, placeholder_output)\n",
    "\n",
    "    init = tf.compat.v1.global_variables_initializer()\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        sess.run(init)\n",
    "\n",
    "        for epoch in range(total_epochs + 1):\n",
    "            random_seed = random_seed + 1\n",
    "            minibatches = random_min_btchs_gcn1(train_input, train_input1, train_output, L_train_input, batch_size, random_seed)\n",
    "            epoch_loss, epoch_accuracy = train_epoch(sess, optimizer_val, loss_val, accuracy_val, minibatches, placeholder_input, placeholder_input1, placeholder_output, laplace_train, training_flag)\n",
    "\n",
    "            if print_loss and (epoch) % 50 == 0:\n",
    "                features, epoch_loss_dev, epoch_accuracy_dev = sess.run(\n",
    "                    [output_input, loss_val, accuracy_val],\n",
    "                    feed_dict={placeholder_input: test_input, placeholder_input1: test_input1, placeholder_output: test_output, laplace_train: L_test_input, training_flag: False}\n",
    "                )\n",
    "                print(f\"Epoch {epoch}: Train_adj_Loss: {epoch_loss}, Val_loss: {epoch_loss_dev}, Train_acc: {epoch_accuracy}, Val_acc: {epoch_accuracy_dev}\")\n",
    "\n",
    "            if print_loss and epoch % 5 == 0:\n",
    "                loss_values.append(epoch_loss)\n",
    "                train_accuracy.append(epoch_accuracy)\n",
    "                loss_values_dev.append(epoch_loss_dev)\n",
    "                validation_accuracy.append(epoch_accuracy_dev)\n",
    "\n",
    "        plot_graphs(loss_values, loss_values_dev, train_accuracy, validation_accuracy)\n",
    "\n",
    "        params = sess.run(params)\n",
    "        return params, validation_accuracy, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_standardize_data():\n",
    "    graph_data_points = sio.loadmat('./cuprite_dataset/data_points_cuprite')['data_points']\n",
    "    graph_data_points = standardize(graph_data_points)\n",
    "\n",
    "    conv_data_points = sio.loadmat('./cuprite_dataset/data_points_CNN_cuprite.mat')['data_points']\n",
    "    conv_data_points = standardize(conv_data_points)\n",
    "\n",
    "    label_data = sio.loadmat('./cuprite_dataset/labels_cuprite.mat')['labels']\n",
    "    label_data = label_data.T\n",
    "    class_count = 12\n",
    "    one_hot_labels = tf.keras.utils.to_categorical(label_data, class_count)\n",
    "\n",
    "    return graph_data_points, conv_data_points, one_hot_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_and_create_adjacency_matrices(graph_data_points, conv_data_points, one_hot_labels):\n",
    "    train_graph_data, test_graph_data, train_conv_data, test_conv_data, train_labels, test_labels = partition_dataset(graph_data_points, conv_data_points, one_hot_labels)\n",
    "\n",
    "    train_adj_matrix = adjacency_matrix(train_graph_data)\n",
    "    train_adj_matrix = train_adj_matrix.astype(int)\n",
    "    train_adj_matrix = train_adj_matrix.todense()\n",
    "\n",
    "    test_adj_matrix = adjacency_matrix(test_graph_data)\n",
    "    test_adj_matrix = test_adj_matrix.astype(int)\n",
    "    test_adj_matrix = test_adj_matrix.todense()\n",
    "\n",
    "    return train_graph_data, test_graph_data, train_conv_data, test_conv_data, train_labels, test_labels, train_adj_matrix, test_adj_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict(train_graph_data, test_graph_data, train_conv_data, test_conv_data, train_labels, test_labels, train_adj_matrix, test_adj_matrix):\n",
    "    trained_params, validation_accuracy, feature_set = train_model(train_graph_data, test_graph_data, train_conv_data, test_conv_data, train_labels, test_labels, train_adj_matrix, test_adj_matrix)\n",
    "\n",
    "    predicted_labels = np.argmax(feature_set, axis=1)\n",
    "    actual_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "    return actual_labels, predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(actual_labels, predicted_labels):\n",
    "    print(\"Accuracy: \", accuracy_score(actual_labels, predicted_labels))\n",
    "    print(\"Precision: \", precision_score(actual_labels, predicted_labels, average=\"macro\"))\n",
    "    print(\"Recall: \", recall_score(actual_labels, predicted_labels, average=\"macro\"))\n",
    "    print(\"F1: \", f1_score(actual_labels, predicted_labels, average=\"macro\"))\n",
    "    print(\"Confusion Matrix: \", confusion_matrix(actual_labels, predicted_labels))\n",
    "    print(\"Classification Report: \", classification_report(actual_labels, predicted_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_data_points, conv_data_points, one_hot_labels = load_and_standardize_data()\n",
    "train_graph_data, test_graph_data, train_conv_data, test_conv_data, train_labels, test_labels, train_adj_matrix, test_adj_matrix = partition_and_create_adjacency_matrices(graph_data_points, conv_data_points, one_hot_labels)\n",
    "actual_labels, predicted_labels = train_and_predict(train_graph_data, test_graph_data, train_conv_data, test_conv_data, train_labels, test_labels, train_adj_matrix, test_adj_matrix)\n",
    "print_metrics(actual_labels, predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
